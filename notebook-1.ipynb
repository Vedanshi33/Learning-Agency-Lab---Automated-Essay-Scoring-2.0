{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-28T09:54:19.451449Z","iopub.execute_input":"2024-06-28T09:54:19.451979Z","iopub.status.idle":"2024-06-28T09:54:19.468134Z","shell.execute_reply.started":"2024-06-28T09:54:19.45193Z","shell.execute_reply":"2024-06-28T09:54:19.46692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:54:19.470503Z","iopub.execute_input":"2024-06-28T09:54:19.470971Z","iopub.status.idle":"2024-06-28T09:54:19.479173Z","shell.execute_reply.started":"2024-06-28T09:54:19.470938Z","shell.execute_reply":"2024-06-28T09:54:19.478011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloads","metadata":{}},{"cell_type":"code","source":"nltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:54:19.480676Z","iopub.execute_input":"2024-06-28T09:54:19.481135Z","iopub.status.idle":"2024-06-28T09:54:19.741833Z","shell.execute_reply.started":"2024-06-28T09:54:19.481099Z","shell.execute_reply":"2024-06-28T09:54:19.740436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataSet","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\ntest=pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\nsample=pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:54:19.743224Z","iopub.execute_input":"2024-06-28T09:54:19.743624Z","iopub.status.idle":"2024-06-28T09:54:20.840461Z","shell.execute_reply.started":"2024-06-28T09:54:19.743581Z","shell.execute_reply":"2024-06-28T09:54:20.839309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA and Findings","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:54:20.843718Z","iopub.execute_input":"2024-06-28T09:54:20.844708Z","iopub.status.idle":"2024-06-28T09:54:20.868155Z","shell.execute_reply.started":"2024-06-28T09:54:20.844661Z","shell.execute_reply":"2024-06-28T09:54:20.866944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:54:20.86944Z","iopub.execute_input":"2024-06-28T09:54:20.869816Z","iopub.status.idle":"2024-06-28T09:54:20.877673Z","shell.execute_reply.started":"2024-06-28T09:54:20.869784Z","shell.execute_reply":"2024-06-28T09:54:20.876536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"change the words to lower case","metadata":{}},{"cell_type":"code","source":"train['full_text'] = train['full_text'].str.lower()\ntest['full_text'] = test['full_text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:54:20.878985Z","iopub.execute_input":"2024-06-28T09:54:20.879359Z","iopub.status.idle":"2024-06-28T09:54:21.020198Z","shell.execute_reply.started":"2024-06-28T09:54:20.879329Z","shell.execute_reply":"2024-06-28T09:54:21.018781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"removing stop words\n","metadata":{}},{"cell_type":"code","source":"# Function to remove stopwords\ndef remove_stopwords(text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n    return ' '.join(filtered_text)\n\n# Apply function to 'full_text' column\ntrain['full_text'] = train['full_text'].apply(remove_stopwords)\ntest['full_text'] = test['full_text'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:54:21.021936Z","iopub.execute_input":"2024-06-28T09:54:21.02234Z","iopub.status.idle":"2024-06-28T09:56:01.398196Z","shell.execute_reply.started":"2024-06-28T09:54:21.022294Z","shell.execute_reply":"2024-06-28T09:56:01.396914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:56:01.399698Z","iopub.execute_input":"2024-06-28T09:56:01.400101Z","iopub.status.idle":"2024-06-28T09:56:01.414362Z","shell.execute_reply.started":"2024-06-28T09:56:01.400057Z","shell.execute_reply":"2024-06-28T09:56:01.412832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"removing all the special characters","metadata":{}},{"cell_type":"code","source":"def remove_special_char(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\ntrain[\"full_text\"]=train[\"full_text\"].apply(remove_special_char)\ntest[\"full_text\"]=test[\"full_text\"].apply(remove_special_char)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:56:01.416771Z","iopub.execute_input":"2024-06-28T09:56:01.417815Z","iopub.status.idle":"2024-06-28T09:56:02.403936Z","shell.execute_reply.started":"2024-06-28T09:56:01.417765Z","shell.execute_reply":"2024-06-28T09:56:02.402714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lemmatization of words\n Error","metadata":{}},{"cell_type":"code","source":"\"\"\"import nltk\nnltk.download('wordnet')\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n    \n    lemmatizer = WordNetLemmatizer()\n    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n    \n    return ' '.join(lemmatized_text)\n\ntrain['full_text'] = train['full_text'].apply(preprocess_text)\ntest['full_text'] = test['full_text'].apply(preprocess_text)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:56:02.405673Z","iopub.execute_input":"2024-06-28T09:56:02.406175Z","iopub.status.idle":"2024-06-28T09:56:02.415142Z","shell.execute_reply.started":"2024-06-28T09:56:02.406132Z","shell.execute_reply":"2024-06-28T09:56:02.413813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"stemming","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\n\ndef preprocess_text(text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n    \n    stemmer = PorterStemmer()\n    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n    \n    return ' '.join(stemmed_text)\n\ntrain['full_text'] = train['full_text'].apply(preprocess_text)\ntest['full_text'] = test['full_text'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:56:02.416604Z","iopub.execute_input":"2024-06-28T09:56:02.416953Z","iopub.status.idle":"2024-06-28T09:59:01.105056Z","shell.execute_reply.started":"2024-06-28T09:56:02.416923Z","shell.execute_reply":"2024-06-28T09:59:01.102873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for frequent words","metadata":{}},{"cell_type":"code","source":"\"\"\"from collections import Counter\n\n# Combine all preprocessed text data into one string\ntext_data = ' '.join(train['full_text'].astype(str).tolist())\n\n# Tokenize the preprocessed text (assuming each word is already stemmed and cleaned)\ntokens = text_data.split()\n\n# Count the frequency of each word\nword_freq = Counter(tokens)\n\n# Get the most common words\nmost_common_words = word_freq.most_common()\n\n# Print the most common words\nprint(most_common_words)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.107628Z","iopub.execute_input":"2024-06-28T09:59:01.108155Z","iopub.status.idle":"2024-06-28T09:59:01.117188Z","shell.execute_reply.started":"2024-06-28T09:59:01.108108Z","shell.execute_reply":"2024-06-28T09:59:01.115812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"removing top 20% frequent words","metadata":{}},{"cell_type":"code","source":"\"\"\"# Combine all preprocessed text data into one string\ntext_data = ' '.join(train['full_text'].astype(str).tolist())\n\n# Tokenize the preprocessed text\ntokens = text_data.split()\n\n# Count the frequency of each word\nword_freq = Counter(tokens)\n\n# Calculate the number of top frequent words to remove (20%)\nnum_words_to_remove = int(len(word_freq) * 0.20)\n\n# Get the most common words to remove\nmost_common_words = [word for word, freq in word_freq.most_common(num_words_to_remove)]\n\n# Define a function to remove the most common words from a text\ndef remove_frequent_words(text, words_to_remove):\n    return ' '.join([word for word in text.split() if word not in words_to_remove])\n\n# Apply the function to remove the most common words from the dataset\ntrain['full_text'] = train['full_text'].apply(lambda x: remove_frequent_words(x, most_common_words))\ntest['full_text'] = test['full_text'].apply(lambda x: remove_frequent_words(x, most_common_words))\n\n# Print a sample to verify the result\nprint(train['full_text'].head())\nprint(test['full_text'].head())\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.126051Z","iopub.execute_input":"2024-06-28T09:59:01.126588Z","iopub.status.idle":"2024-06-28T09:59:01.141554Z","shell.execute_reply.started":"2024-06-28T09:59:01.126544Z","shell.execute_reply":"2024-06-28T09:59:01.139324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For rare words","metadata":{}},{"cell_type":"code","source":"\"\"\"from collections import Counter\n\n# Combine all preprocessed text data into one string\ntext_data = ' '.join(train['full_text'].astype(str).tolist())\n\n# Tokenize the preprocessed text\ntokens = text_data.split()\n\n# Count the frequency of each word\nword_freq = Counter(tokens)\n\n# Calculate the number of least frequent words to remove (20%)\nnum_words_to_remove = int(len(word_freq) * 0.20)\n\n# Get the least common words to remove\nleast_common_words = [word for word, freq in word_freq.most_common()[:-num_words_to_remove-1:-1]]\n\n# Define a function to remove the least common words from a text\ndef remove_rare_words(text, words_to_remove):\n    return ' '.join([word for word in text.split() if word not in words_to_remove])\n\n# Apply the function to remove the least common words from the dataset\ntrain['full_text'] = train['full_text'].apply(lambda x: remove_rare_words(x, least_common_words))\ntest['full_text'] = test['full_text'].apply(lambda x: remove_rare_words(x, least_common_words))\n\n# Print a sample to verify the result\nprint(train['full_text'].head())\nprint(test['full_text'].head())\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.143469Z","iopub.execute_input":"2024-06-28T09:59:01.144037Z","iopub.status.idle":"2024-06-28T09:59:01.165953Z","shell.execute_reply.started":"2024-06-28T09:59:01.143989Z","shell.execute_reply":"2024-06-28T09:59:01.163669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train['full_text']\ny = train['score']\n\n# Split the train data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.167471Z","iopub.execute_input":"2024-06-28T09:59:01.167975Z","iopub.status.idle":"2024-06-28T09:59:01.198739Z","shell.execute_reply.started":"2024-06-28T09:59:01.167936Z","shell.execute_reply":"2024-06-28T09:59:01.197124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.200619Z","iopub.execute_input":"2024-06-28T09:59:01.201265Z","iopub.status.idle":"2024-06-28T09:59:01.213091Z","shell.execute_reply.started":"2024-06-28T09:59:01.201181Z","shell.execute_reply":"2024-06-28T09:59:01.210878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(X_train) #pandas.core.series.Series\nX_train","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.215413Z","iopub.execute_input":"2024-06-28T09:59:01.216189Z","iopub.status.idle":"2024-06-28T09:59:01.23527Z","shell.execute_reply.started":"2024-06-28T09:59:01.216096Z","shell.execute_reply":"2024-06-28T09:59:01.233251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"logistic regression","metadata":{}},{"cell_type":"markdown","source":"Accuracy: 0.47631426920854997","metadata":{}},{"cell_type":"code","source":"\"\"\"pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('classifier',LogisticRegression(max_iter=1000, random_state=42))\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)\n\n# Predict on the validation set\ny_pred = pipeline.predict(X_test)\n\n# Calculate accuracy as a metric\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\"\"\"\n# Now, let's predict on the test data\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.237151Z","iopub.execute_input":"2024-06-28T09:59:01.237581Z","iopub.status.idle":"2024-06-28T09:59:01.254889Z","shell.execute_reply.started":"2024-06-28T09:59:01.237545Z","shell.execute_reply":"2024-06-28T09:59:01.253284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"random forest","metadata":{}},{"cell_type":"markdown","source":"accuracy 0.49","metadata":{}},{"cell_type":"code","source":"\"\"\"\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)\n\n# Predict on the validation set\ny_pred = pipeline.predict(X_valid)\n\n# Calculate accuracy as a metric\naccuracy = accuracy_score(y_valid, y_pred)\nprint(\"Accuracy:\", accuracy)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.256972Z","iopub.execute_input":"2024-06-28T09:59:01.257492Z","iopub.status.idle":"2024-06-28T09:59:01.27305Z","shell.execute_reply.started":"2024-06-28T09:59:01.257436Z","shell.execute_reply":"2024-06-28T09:59:01.270812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# gradient boost (best performing so far)","metadata":{}},{"cell_type":"code","source":"# This the BEST CLASSIFIER SO FAR WITH TEST CLASSSIFICATON SCORE OF 0.65\n# Create a pipeline with TF-IDF vectorizer and Gradient Boosting Classifier\n\"\"\"pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)\n\n# Predict on the validation set\ny_pred = pipeline.predict(X_test)\n\n# Calculate accuracy as a metric\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.2754Z","iopub.execute_input":"2024-06-28T09:59:01.276138Z","iopub.status.idle":"2024-06-28T09:59:01.296973Z","shell.execute_reply.started":"2024-06-28T09:59:01.2761Z","shell.execute_reply":"2024-06-28T09:59:01.29447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dummy submission","metadata":{}},{"cell_type":"code","source":"\"\"\"from sklearn.dummy import DummyClassifier\n\n#Load the training data\n#df = pd.read_csv(\"your_training_data.csv\")\n\n#Assuming 'rating' is the target variable\nX = train.drop(\"score\", axis=1)\ny = train[\"score\"]\n\n#Create a DummyClassifier model\nmodel = DummyClassifier(strategy=\"most_frequent\")  # Replace 'your_strategy' with the desired strategy\n\n#Fit the model to the training data\nmodel.fit(X, y)\n\n#Load the test data\n#Make predictions on the test data\ny_pred = model.predict(test)\n\n#Create a submission DataFrame\nsubmission = pd.DataFrame({\"essay_id\": test['essay_id'], \"score\": y_pred})\n\n#Save the submission to a CSV file\nsubmission.to_csv('submission.csv', index=False)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:01.299Z","iopub.execute_input":"2024-06-28T09:59:01.299541Z","iopub.status.idle":"2024-06-28T09:59:01.317461Z","shell.execute_reply.started":"2024-06-28T09:59:01.299473Z","shell.execute_reply":"2024-06-28T09:59:01.315681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying xg_boost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score\n!pip install imbalanced-learn\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\n# Adjust class labels\ny_train_for_XG = y_train - 1\ny_test_for_XG = y_test - 1\n\n# Define the pipeline with TF-IDF, SMOTE, and XGBoost classifier\npipeline = ImbPipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('smote', SMOTE(random_state=42)),\n    ('xgb_classifier', xgb.XGBClassifier(objective='multi:softmax', num_class=6))\n])\n\n# Fit the training data\npipeline.fit(X_train, y_train_for_XG)\n\n# Predict on the validation set\ny_pred = pipeline.predict(X_test)\n\n# Calculate accuracy as a metric\naccuracy = accuracy_score(y_test_for_XG, y_pred)\nprint(\"Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T10:26:07.064968Z","iopub.execute_input":"2024-06-28T10:26:07.065403Z","iopub.status.idle":"2024-06-28T10:34:46.701054Z","shell.execute_reply.started":"2024-06-28T10:26:07.065359Z","shell.execute_reply":"2024-06-28T10:34:46.699861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"xgboost with sentence transformer","metadata":{}},{"cell_type":"code","source":"\"\"\"# Install the sentence-transformers library\n!pip install -q sentence-transformers\n\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the SentenceTransformer model\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n# Generate embeddings for each essay in the training set\nembeddings_train = model.encode(X_train.tolist(), convert_to_tensor=True)\nembeddings_test = model.encode(X_test.tolist(), convert_to_tensor=True)\n\n# Convert embeddings to numpy arrays\nembeddings_train_np = embeddings_train.cpu().detach().numpy()\nembeddings_test_np = embeddings_test.cpu().detach().numpy()\n\n# Adjust labels for XGBoost (assuming labels start from 1)\ny_train_for_XG = y_train - 1\ny_test_for_XG = y_test - 1\n\n# Define the XGBoost classifier\nxgb_classifier = XGBClassifier(objective='multi:softmax', num_class=6)\n\n# Fit the classifier on the training data\nxgb_classifier.fit(embeddings_train_np, y_train_for_XG)\n\n# Predict on the test set\ny_pred = xgb_classifier.predict(embeddings_test_np)\n\n# Calculate accuracy as a metric\naccuracy = accuracy_score(y_test_for_XG, y_pred)\nprint(\"Accuracy:\", accuracy)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:20.352725Z","iopub.status.idle":"2024-06-28T09:59:20.353154Z","shell.execute_reply.started":"2024-06-28T09:59:20.352953Z","shell.execute_reply":"2024-06-28T09:59:20.352972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique, counts = np.unique(y_test_for_XG, return_counts=True)\ndict(zip(unique, counts))","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:20.354996Z","iopub.status.idle":"2024-06-28T09:59:20.35565Z","shell.execute_reply.started":"2024-06-28T09:59:20.355323Z","shell.execute_reply":"2024-06-28T09:59:20.355351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = pipeline.predict(test['full_text']) + 1\n\n# Create a submission DataFrame\nsubmission = pd.DataFrame({\"essay_id\": test['essay_id'], \"score\": test_pred})\n\n# Save the submission to a CSV file\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:20.357105Z","iopub.status.idle":"2024-06-28T09:59:20.357733Z","shell.execute_reply.started":"2024-06-28T09:59:20.357412Z","shell.execute_reply":"2024-06-28T09:59:20.357437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_pred = pipeline.predict(test['full_text'])\n\n# # Create a submission DataFrame\n# submission = pd.DataFrame({\"essay_id\": test['essay_id'], \"score\": test_pred})\n\n# # Save the submission to a CSV file\n# submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:20.359002Z","iopub.status.idle":"2024-06-28T09:59:20.359397Z","shell.execute_reply.started":"2024-06-28T09:59:20.35921Z","shell.execute_reply":"2024-06-28T09:59:20.359227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:59:20.361728Z","iopub.status.idle":"2024-06-28T09:59:20.362155Z","shell.execute_reply.started":"2024-06-28T09:59:20.361957Z","shell.execute_reply":"2024-06-28T09:59:20.361974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}